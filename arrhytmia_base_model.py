# -*- coding: utf-8 -*-
"""Pytorch classifier Optuna Tuner

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YNs9xsXrEZoyo2MfqiYkOh49WSq_Xp0T

#Description of the Notebook

This Google Colab Notebook was created for the creation of a dense neural network using Pytorch. It utilizes Optuna in order to determine the optimal hyperparameters.

# Import Data and Metadata
"""

# Import data acquisition libraries
import pandas as pd
import requests as r
#Import data processing libraries
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import colors
import os
from torch.utils.data.dataset import Dataset

#data import

arrhythmia_data = 'https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.data'
arrhythmia_classes = 'http://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.names'
dataset = pd.read_csv(arrhythmia_data,header = None)

names = r.get(arrhythmia_classes).content.decode('utf-8')

names

#Replacement of missing values with median value for that attribute
dataset = dataset.replace('?', np.nan)
dataset.fillna(dataset.median(), inplace = True)
dataset[279] = dataset[279]-1

"""#Creation of the Pytorch Dataset and Model"""


# Import Neural Network Libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torch.utils.data import Dataset, random_split
import pickle

import optuna
from optuna.trial import TrialState
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn import preprocessing


# Dataloader for pandas dataframe to pytorch dataset conversion
class pandas_dataset(Dataset):

  def __init__(self,pd_dataframe):
    df= pd_dataframe
    
    x=df.iloc[:,:-1].values
    y=df.iloc[:,-1:].values
    transformer = preprocessing.RobustScaler().fit(x)
    x = transformer.transform(x)

    self.x_train=torch.tensor(x.astype(np.float32),dtype=torch.float32)
    self.y_train=torch.tensor(y.astype(np.int8),dtype=torch.long)
    y = torch.tensor(y[:, 0])

  def __len__(self):
    return len(self.y_train)
  
  def __getitem__(self,idx):
    return self.x_train[idx],self.y_train[idx]

df_dataset=pandas_dataset(dataset)
#data_loader=DataLoader(df_dataset,batch_size=8,shuffle=False)

train, test = random_split(df_dataset,[400,52])

use_cuda = torch.cuda.is_available()
device = torch.device("cuda:0" if use_cuda else "cpu")
torch.backends.cudnn.benchmark = True
batch_size = 8
params = {'batch_size': batch_size,
          'shuffle': True,
          'num_workers': 2}

train_loader = DataLoader(train, **params)
test_loader = DataLoader(test,**params)

n_features = len(dataset.columns) - 1
n_classes = 16
n_epochs = 25
n_neurons = [128, 64, 32]

criterion = nn.CrossEntropyLoss()

LOG_INTERVAL = 10
N_TRAIN_EXAMPLES = batch_size * 1000
N_VALID_EXAMPLES = batch_size * 1000
EPOCHS = 100

def optuna_model(trial):
    # We optimize the number of layers, hidden units and dropout ratio in each layer.
    n_layers = trial.suggest_int("n_layers", 1, 4)
    layers = []

    in_features = n_features
    for i in range(n_layers):
        out_features = trial.suggest_int("n_units_l{}".format(i), 4, 128)
        layers.append(nn.Linear(in_features, out_features))
        layers.append(nn.ReLU())
        p = trial.suggest_float("dropout_l{}".format(i), 0.1, 0.5)
        layers.append(nn.Dropout(p))

        in_features = out_features
    layers.append(nn.Linear(in_features, n_classes))
    layers.append(nn.LogSoftmax(dim=1))

    return nn.Sequential(*layers)

def objective(trial):

    # Generate the model.
    model = optuna_model(trial).to(device)

    # Generate the optimizers.
    optimizer_name = trial.suggest_categorical("optimizer", ["Adam", "RMSprop", "SGD"])
    lr = trial.suggest_float("lr", 1e-5, 1e-1, log=True)
    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)


    # Training of the model.
    for epoch in range(EPOCHS):
        model.train()
        for batch_idx, (data, target) in enumerate(train_loader):
            # Limiting training data for faster epochs.
            if batch_idx * batch_size >= N_TRAIN_EXAMPLES:
                break

            data, target = data.view(data.size(0), -1).to(device), target.flatten().to(device)

            optimizer.zero_grad()
            output = model(data)
            loss = F.nll_loss(output, target)
            loss.backward()
            optimizer.step()

        # Validation of the model.
        model.eval()
        correct = 0
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(test_loader):
                # Limiting validation data.
                if batch_idx * batch_size >= N_VALID_EXAMPLES:
                    break
                data, target = data.view(data.size(0), -1).to(device), target.to(device)
                output = model(data)
                # Get the index of the max log-probability.
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()

        accuracy = correct / min(len(test_loader.dataset), N_VALID_EXAMPLES)

        trial.report(accuracy, epoch)

        with open("{}.pickle".format(trial.number), "wb") as fout:
          pickle.dump(model, fout)

        # Handle pruning based on the intermediate value.
        if trial.should_prune():
            raise optuna.exceptions.TrialPruned()

    return accuracy

if __name__ == "__main__":
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=100, timeout=60000)

    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])
    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])

    print("Study statistics: ")
    print("  Number of finished trials: ", len(study.trials))
    print("  Number of pruned trials: ", len(pruned_trials))
    print("  Number of complete trials: ", len(complete_trials))

    print("Best trial:")
    trial = study.best_trial

    print("  Value: ", trial.value)

    print("  Params: ")
    for key, value in trial.params.items():
        print("    {}: {}".format(key, value))
